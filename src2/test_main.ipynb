{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src2.transformations import transformation_of_output_summary\n",
    "from src2.load_glove import load_global_vectors\n",
    "from src2.word_vector_conversions import vector_to_word\n",
    "from src2.model import model\n",
    "import numpy as np\n",
    "import pickle\n",
    "import string\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for cross-file access\n",
    "vocabulary = None\n",
    "positions = None\n",
    "embeddings = None\n",
    "dimension_of_word_vector = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe Loading Complete!\n"
     ]
    }
   ],
   "source": [
    "# GloVe file name\n",
    "glove_file_name = '../glove.6B/glove.6B.50d.txt'\n",
    "\n",
    "# load the glove file\n",
    "vocabulary, positions = load_global_vectors(glove_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert positions to np array and change their data-type to float32\n",
    "embeddings = np.asarray(positions)\n",
    "embeddings = embeddings.astype(np.float32)\n",
    "\n",
    "# The dimensions of all vectors will be same, so we just use the 1st vector\n",
    "# to find the dimensions\n",
    "dimension_of_word_vector = len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is the pickled binary file of summaries in vector form which\n",
    "# we un-pickle now\n",
    "with open('../processed_data/amazon_reviews/_summaries_in_vector_form', 'rb') as fp:\n",
    "    _summaries_in_vector_form = pickle.load(fp)\n",
    "\n",
    "# The following is the pickled binary file of texts in vector form which\n",
    "# we un-pickle now\n",
    "with open('../processed_data/amazon_reviews/_texts_in_vector_form', 'rb') as fp:\n",
    "    _texts_in_vector_form = pickle.load(fp)\n",
    "\n",
    "# The following is the pickled binary file of vocabulary of reviews\n",
    "# in vector form which we un-pickle now\n",
    "with open('../processed_data/amazon_reviews/_reviews_vocabulary', 'rb') as fp:\n",
    "    _reviews_vocabulary = pickle.load(fp)\n",
    "\n",
    "# The following is the pickled binary file of embeddings(positions)\n",
    "# of reviews vocabulary(in vector form) which we un-pickle now\n",
    "with open('../processed_data/amazon_reviews/_reviews_embeddings', 'rb') as fp:\n",
    "    _reviews_embeddings = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOS -> start of sentence token, which is also added to our vocabulary\n",
    "_reviews_vocabulary.append('<SOS>')\n",
    "\n",
    "# Its position is a vector with values as 0 so that it has no effect on the\n",
    "# processing of the data\n",
    "_SOS_position = np.zeros(dimension_of_word_vector, dtype=np.float32)\n",
    "\n",
    "# then we append its position to the embeddings as well\n",
    "_reviews_embeddings.append(_SOS_position)\n",
    "\n",
    "# numpy array format of the review embeddings\n",
    "_np_reviews_embeddings = np.asarray(_reviews_embeddings, dtype=np.float32)\n",
    "\n",
    "# 80 percent of data for training\n",
    "_n_percent = 80\n",
    "\n",
    "# n% of the total data will be used for training\n",
    "_length_of_train_data = int(len(_texts_in_vector_form) * _n_percent / 100)\n",
    "\n",
    "# choosing the training texts\n",
    "_train_texts_in_vector_form = _texts_in_vector_form[:_length_of_train_data]\n",
    "# choosing the training summaries\n",
    "_train_summaries_in_vector_form = _summaries_in_vector_form[:_length_of_train_data]\n",
    "\n",
    "# choosing the test texts\n",
    "_test_texts_in_vector_form = _texts_in_vector_form[_length_of_train_data:]\n",
    "# choosing the test summaries\n",
    "_test_summaries_in_vector_form = _summaries_in_vector_form[_length_of_train_data:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# range_width is a hyper-parameter:\n",
    "# (parameter whose value is set before the learning process begins).\n",
    "# Windows size for local attention will be (2 * range_width) + 1\n",
    "range_width = 10\n",
    "\n",
    "# window: let the current phrase under processing be p. We have a range\n",
    "# width of 10. So, we will use local attention for all points from p - 10 to\n",
    "# p + 10. Window is all the points. hence window size will be, 10 points before\n",
    "# p, p, and 10 points after p = 10 + 1 + 10 = 2 * 10 + 1 = 2 * range_width + 1\n",
    "window_size = 2 * range_width + 1\n",
    "\n",
    "# Removing all the summaries that have length, greater than window size\n",
    "# Removing all the texts that have length less than window size or greater than\n",
    "# max_allowed_text_length\n",
    "MAX_ALLOWED_LENGTH_OF_TEXT = 80  # arbitrary\n",
    "MAX_ALLOWED_LENGTH_OF_SUMMARY = 7  # arbitrary\n",
    "\n",
    "# Storing their values in a temp variable\n",
    "_temp_summaries_in_vector_form = _summaries_in_vector_form\n",
    "_temp_texts_in_vector_form = _texts_in_vector_form\n",
    "\n",
    "# We initialize them as empty lists, so we can select only those that\n",
    "# meet our length requirements\n",
    "_summaries_in_vector_form = []\n",
    "_texts_in_vector_form = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We iterate over all the summaries\n",
    "for i, _summary in enumerate(_temp_summaries_in_vector_form, 0):\n",
    "\n",
    "    # if the length of the summary is less than max_allowed_length_for_summary\n",
    "    # and the length of the corresponding text lies in window_size\n",
    "    # and max_allowed_length_for_text\n",
    "    if len(_summary) <= MAX_ALLOWED_LENGTH_OF_SUMMARY\\\n",
    "            and window_size <= len(_temp_texts_in_vector_form[i])\\\n",
    "            <= MAX_ALLOWED_LENGTH_OF_TEXT:\n",
    "\n",
    "        # We select the summary and its corresponding text\n",
    "        _summaries_in_vector_form.append(_summary)\n",
    "        _texts_in_vector_form.append(_temp_texts_in_vector_form[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual beginning of training\n",
    "\n",
    "# number of hidden layer neurons\n",
    "_size_of_hidden_layer = 500\n",
    "\n",
    "# how much each iteration affects the weights\n",
    "_learning_rate = 0.003\n",
    "\n",
    "# no. of previous hidden states to consider for residual connections.\n",
    "# special hyper-parameter\n",
    "_K = 5\n",
    "\n",
    "# quite obvious\n",
    "_length_of_vocabulary = len(_reviews_vocabulary)\n",
    "# again, quite obvious\n",
    "_training_iterations = 5\n",
    "\n",
    "# initializing some place-holders\n",
    "_tf_text = tf.placeholder(tf.float32, [None, dimension_of_word_vector])\n",
    "_tf_length_of_sequence = tf.placeholder(tf.int32)\n",
    "_tf_summary = tf.placeholder(tf.int32, [None])\n",
    "_tf_length_of_output = tf.placeholder(tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/apple/PycharmProjects/AutomaticTextSummarizer/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/apple/PycharmProjects/AutomaticTextSummarizer/venv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "_output = model(_tf_text, _tf_length_of_sequence, _tf_length_of_output,\n",
    "                dimension_of_word_vector, _length_of_vocabulary, _np_reviews_embeddings,\n",
    "                _SOS_position)\n",
    "\n",
    "# Optimizer and cost\n",
    "_cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    logits=_output, labels=_tf_summary))\n",
    "_optimizer = tf.train.AdamOptimizer(learning_rate=_learning_rate).minimize(_cost)\n",
    "\n",
    "# prediction\n",
    "_prediction = tf.TensorArray(size=_tf_length_of_output, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the body\n",
    "def _body_prediction(__i, __prediction):\n",
    "    __prediction = __prediction.write(__i, tf.cast(tf.argmax(_output[__i]), tf.int32))\n",
    "    return __i + 1, __prediction\n",
    "\n",
    "_, _prediction = tf.while_loop(lambda __i, _a1: __i < _tf_length_of_output,\n",
    "                               _body_prediction,\n",
    "                               [0, _prediction])\n",
    "\n",
    "_prediction = _prediction.stack()\n",
    "\n",
    "# initialize global variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 0\n",
      "Training input sequence length: 6\n",
      "Training target outputs sequence length: 6\n",
      "\n",
      "TEXT:\n",
      "< filter object at unk>\n",
      "\n"
     ]
    },
    {
     "ename": "AlreadyExistsError",
     "evalue": "Resource __per_step_2/while/ArithmeticOptimizer/AddOpsRewrite_add_4/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\n\t [[{{node while/ArithmeticOptimizer/AddOpsRewrite_add_4/tmp_var}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAlreadyExistsError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/Users/apple/PycharmProjects/AutomaticTextSummarizer/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apple/PycharmProjects/AutomaticTextSummarizer/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apple/PycharmProjects/AutomaticTextSummarizer/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAlreadyExistsError\u001b[0m: Resource __per_step_2/while/ArithmeticOptimizer/AddOpsRewrite_add_4/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\n\t [[{{node while/ArithmeticOptimizer/AddOpsRewrite_add_4/tmp_var}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAlreadyExistsError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8f41d110f5fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m                                          \u001b[0m_tf_text\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_train_texts_in_vector_form\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                                          \u001b[0m_tf_length_of_sequence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_train_texts_in_vector_form\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                                          \u001b[0m_tf_summary\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_tf_length_of_output\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                                      })\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apple/PycharmProjects/AutomaticTextSummarizer/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apple/PycharmProjects/AutomaticTextSummarizer/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apple/PycharmProjects/AutomaticTextSummarizer/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/apple/PycharmProjects/AutomaticTextSummarizer/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAlreadyExistsError\u001b[0m: Resource __per_step_2/while/ArithmeticOptimizer/AddOpsRewrite_add_4/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE\n\t [[{{node while/ArithmeticOptimizer/AddOpsRewrite_add_4/tmp_var}}]]"
     ]
    }
   ],
   "source": [
    "# start session\n",
    "with tf.Session() as sess:  # Start Tensor-flow Session\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    # Prepares variable for saving the model\n",
    "    sess.run(init)  # initialize all variables\n",
    "    step = 0\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    val_loss_list = []\n",
    "    val_acc_list = []\n",
    "    best_val_acc = 0\n",
    "    display_step = 1\n",
    "\n",
    "    while step < _training_iterations:\n",
    "\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        total_val_loss = 0\n",
    "        total_val_acc = 0\n",
    "\n",
    "        for i in range(0, _length_of_train_data):\n",
    "\n",
    "            train_out = transformation_of_output_summary(\n",
    "                _train_summaries_in_vector_form[i][:-1], _reviews_vocabulary, _reviews_embeddings)\n",
    "\n",
    "            if i % display_step == 0:\n",
    "                print(\"\\nIteration: \" + str(i))\n",
    "                print(\"Training input sequence length: \" +\n",
    "                      str(len(_train_texts_in_vector_form[i])))\n",
    "                print(\"Training target outputs sequence length: \" + str(len(train_out)))\n",
    "\n",
    "                print(\"\\nTEXT:\")\n",
    "                flag = 0\n",
    "                for vec in _train_texts_in_vector_form[i]:\n",
    "                    if vector_to_word(vec, _reviews_vocabulary, _reviews_embeddings)\\\n",
    "                            in string.punctuation or flag == 0:\n",
    "                        print(str(vector_to_word(vec, _reviews_vocabulary, _reviews_embeddings)), end='')\n",
    "                    else:\n",
    "                        print((\" \" + str(vector_to_word(vec, _reviews_vocabulary, _reviews_embeddings))), end='')\n",
    "                    flag = 1\n",
    "\n",
    "                print(\"\\n\")\n",
    "\n",
    "            # Run optimization operation (back-propagation)\n",
    "            _, loss, _prediction_ = sess.run([_optimizer, _cost, _prediction],\n",
    "                                             feed_dict={\n",
    "                                         _tf_text: _train_texts_in_vector_form[i],\n",
    "                                         _tf_length_of_sequence: len(_train_texts_in_vector_form[i]),\n",
    "                                         _tf_summary: train_out, _tf_length_of_output: len(train_out)\n",
    "                                     })\n",
    "\n",
    "            if i % display_step == 0:\n",
    "                print(\"\\nPREDICTED SUMMARY:\\n\")\n",
    "                flag = 0\n",
    "                for index in _prediction_:\n",
    "                    # if int(index)!=vocab_limit.index('eos'):\n",
    "                    if _reviews_vocabulary[int(index)] in string.punctuation or flag == 0:\n",
    "                        print(str(_reviews_vocabulary[int(index)]), end='')\n",
    "                    else:\n",
    "                        print(\" \" + str(_reviews_vocabulary[int(index)]), end='')\n",
    "                    flag = 1\n",
    "                print(\"\\n\")\n",
    "\n",
    "                print(\"ACTUAL SUMMARY:\\n\")\n",
    "                flag = 0\n",
    "                for vec in _train_summaries_in_vector_form[i]:\n",
    "                    if vector_to_word(vec, _reviews_vocabulary, _reviews_embeddings) != 'eos':\n",
    "                        if vector_to_word(vec, _reviews_vocabulary, _reviews_embeddings)\\\n",
    "                                in string.punctuation or flag == 0:\n",
    "                            print(str(vector_to_word(vec, _reviews_vocabulary, _reviews_embeddings)), end='')\n",
    "                        else:\n",
    "                            print((\" \" +\n",
    "                                   str(vector_to_word(vec, _reviews_vocabulary, _reviews_embeddings))),\n",
    "                                  end='')\n",
    "                    flag = 1\n",
    "\n",
    "                print(\"\\n\")\n",
    "                print(\"loss=\" + str(loss))\n",
    "\n",
    "        step = step + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
